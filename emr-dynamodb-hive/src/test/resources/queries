CREATE EXTERNAL TABLE Customer (customerId bigint, name string, email String, keys1 array<String>, nullcolumn array<bigint>)
STORED BY 'org.apache.hadoop.hive.dynamodb.DynamoDBStorageHandler'
TBLPROPERTIES ("dynamodb.table.name" = "Customer", 
"dynamodb.column.mapping" =
"customerId:CustomerId,name:Name,email:Email,keys1:Keys,nullcolumn:nullcolumn");

CREATE EXTERNAL TABLE JobFlow(jobFlowId string, customerId bigint, numInstances bigint, cost double, state string, lastStateChanges array<String>, nullcolumn string)
STORED BY 'org.apache.hadoop.hive.dynamodb.DynamoDBStorageHandler'
TBLPROPERTIES ("dynamodb.table.name" = "JobFlow", 
"dynamodb.column.mapping" =
"jobFlowId:Id,customerId:CustomerId,numInstances:NumInstances,cost:Cost,STATE:State,LastStateChanges:LastStateChanges,nullcolumn:nullcolumn");

= Test reading data null data

select * from Jobflow where jobflowid="J-344";
select * from Customer;

= Query pushdown with between
select * from Jobflow where  jobflowId= "J-344" AND customerId >= 1 AND customerId <= 6000 AND customerid > 1230;

= Scan with between
select * from Jobflow where  customerId >= "1" AND customerId <= "5000";

= IN Clause on range key
select * from Jobflow where jobflowId= "J-344" AND customerid in (1234, 5678);

= IN clause on primary key
select * from Jobflow where jobflowid in ("J-206", "J-207", "J-208");

= IN clause on non key column
select * from Jobflow where state in ("ERRORED", "RUNNING");

= Scan with filter pushdown for between
select * from Jobflow where jobflowId > "J-900" AND customerId >= "5000" AND customerId <= "6000" AND state = "RUNNING";

select * from Customer where customerid <> 1234;
= Self Join

select * from JobFlow j1
JOIN Jobflow j2 ON (j1.customerid=j2.customerid) WHERE 
j1.jobflowid = "J-344"; 

= Test <>

select * from Customer where customerid <> 1234;
select * from Jobflow where jobflowid = "J-344" AND customerid <> 5678;

= Test choosing priority filter

select * from Jobflow where jobflowid = "J-344" AND jobflowid >= "J-344" AND jobflowid <= "J-4" AND jobflowid < "J-4";

= Find max

SELECT max(jobFlowId) from JobFlow where state = "RUNNING";

= Group by

SELECT CustomerId, max(cost) from JobFlow GROUP BY customerId HAVING count(*) > 3; 

= Join 2 tables

Select c.customerId, name, count(*) as count from Customer c JOIN JobFlow j ON c.customerid=j.customerid GROUP BY c.customerId, name HAVING count > 2;

Select c.customerId, name, count(*) as count from Customer c JOIN JobFlow j ON c.customerid=j.customerid where c.customerId = "1234" AND j.jobflowid > "J-9" 
GROUP BY c.customerId,name HAVING count > 2;

= Move data to a directory

INSERT OVERWRITE DIRECTORY 's3://vaggarw-org.apache.hadoop.hive/bigbird/directory/' select * from Customer;

= Move data to a s3 table

Create external table Customer_S3
(customerId bigint, name string, email String, keys1 array<String>, nullcolumn array<bigint>)
LOCATION 's3://vaggar-org.apache.hadoop.hive/bigbird/table/';

Insert overwrite table Customer_S3 select * from Customer;

select * from Customer_S3;

= JOIN 2 Tables in Different Data sources

Select JobFlowID, c.CustomerId, name, Email, cost from Customer_S3 c JOIN JobFlow j 
ON c.customerid=j.customerid where State like '%ERROR%';

= JOIN 2 tables with same column name filter pushdown

select * from Customer c JOIN JobFlow j ON c.customerid=j.customerid WHERE c.customerId >= "5678" AND j.customerId <= "5678";